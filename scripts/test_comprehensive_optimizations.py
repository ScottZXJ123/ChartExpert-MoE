#!/usr/bin/env python3
"""
ç»¼åˆä¼˜åŒ–æµ‹è¯•è„šæœ¬
æµ‹è¯•ChartExpert-MoEçš„æ‰€æœ‰æ¶æ„ä¼˜åŒ–å’Œæ€§èƒ½æå‡

åŒ…å«æµ‹è¯•ï¼š
1. å±‚æ¬¡åŒ–ä¸“å®¶æ¶æ„
2. åŠ¨æ€ä¸“å®¶é€‰æ‹©æœºåˆ¶  
3. ä¸“å®¶è®¡ç®—çš„æ‰¹å¤„ç†ä¼˜åŒ–
4. æ”¹è¿›çš„å†…å­˜ç®¡ç†
5. æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–
6. æ¨ç†åŠ é€Ÿï¼ˆKVç¼“å­˜å’Œæ—©æœŸé€€å‡ºï¼‰
7. ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import torch
import torch.nn.functional as F
import numpy as np
import time
import psutil
import gc
from typing import Dict, List, Tuple, Any
import json
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.models.chart_expert_moe import ChartExpertMoE
from src.utils.memory_manager import AdaptiveMemoryManager
from src.fusion.multimodal_fusion import ChartAwareAttention


class ComprehensiveOptimizationTester:
    """ç»¼åˆä¼˜åŒ–æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.results = {}
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            "hidden_size": 768,
            "vocab_size": 32000,
            "num_heads": 12,
            "use_hierarchical_experts": True,
            "use_flash_attention": True,
            "simple_confidence_threshold": 0.95,
            "medium_confidence_threshold": 0.90,
            "complex_confidence_threshold": 0.85,
            "num_early_exit_layers": 3,
            "kv_cache_size_limit": 1024 * 1024 * 1024,  # 1GB
            "vision_encoder": {"hidden_size": 768},
            "llm_backbone": {"hidden_size": 4096},
            "fusion": {"hidden_size": 768, "num_heads": 12},
            "routing": {"hidden_size": 768, "num_experts": 12},
            "moe": {"hidden_size": 768},
            "experts": {
                "layout": {"hidden_size": 768},
                "ocr": {"hidden_size": 768},
                "scale": {"hidden_size": 768},
                "geometric": {"hidden_size": 768},
                "trend": {"hidden_size": 768},
                "query": {"hidden_size": 768},
                "numerical": {"hidden_size": 768},
                "integration": {"hidden_size": 768},
                "alignment": {"hidden_size": 768},
                "chart_to_graph": {"hidden_size": 768},
                "shallow_reasoning": {"hidden_size": 768},
                "orchestrator": {"hidden_size": 768}
            }
        }
        
        print(f"ğŸš€ åˆå§‹åŒ–ç»¼åˆä¼˜åŒ–æµ‹è¯•å™¨")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        print(f"ğŸ§  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB" if torch.cuda.is_available() else "CPUæ¨¡å¼")
        
    def create_test_model(self) -> ChartExpertMoE:
        """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
        try:
            model = ChartExpertMoE(self.test_config)
            model = model.to(self.device)
            model.eval()
            return model
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def generate_test_data(self, batch_size: int = 2, seq_len: int = 64) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        # å›¾åƒæ•°æ® (æ¨¡æ‹Ÿå›¾è¡¨å›¾åƒ)
        image = torch.randn(batch_size, 3, 224, 224, device=self.device)
        
        # æ–‡æœ¬æ•°æ® (æ¨¡æ‹ŸæŸ¥è¯¢)
        input_ids = torch.randint(0, self.test_config["vocab_size"], (batch_size, seq_len), device=self.device)
        
        # æ³¨æ„åŠ›æ©ç 
        attention_mask = torch.ones(batch_size, seq_len, device=self.device)
        
        return image, input_ids, attention_mask
    
    def test_hierarchical_experts(self, model: ChartExpertMoE) -> Dict[str, Any]:
        """æµ‹è¯•å±‚æ¬¡åŒ–ä¸“å®¶æ¶æ„"""
        print("\nğŸ”„ æµ‹è¯•å±‚æ¬¡åŒ–ä¸“å®¶æ¶æ„...")
        
        test_results = {
            "hierarchical_enabled": hasattr(model, 'expert_levels'),
            "num_levels": 0,
            "experts_per_level": [],
            "adaptive_selection_working": False,
            "performance": {}
        }
        
        if not test_results["hierarchical_enabled"]:
            print("âš ï¸  å±‚æ¬¡åŒ–ä¸“å®¶æœªå¯ç”¨")
            return test_results
        
        test_results["num_levels"] = len(model.expert_levels)
        test_results["experts_per_level"] = [len(level) for level in model.expert_levels]
        
        # æµ‹è¯•è‡ªé€‚åº”ä¸“å®¶é€‰æ‹©
        try:
            image, input_ids, attention_mask = self.generate_test_data()
            
            # æµ‹è¯•ä¸åŒå¤æ‚åº¦
            complexities = [torch.tensor([[0.2]]), torch.tensor([[0.6]]), torch.tensor([[0.9]])]
            complexity_names = ["simple", "medium", "complex"]
            
            for complexity, name in zip(complexities, complexity_names):
                expert_counts = model._adaptive_expert_count(complexity)
                test_results["performance"][f"{name}_expert_counts"] = expert_counts
                
            test_results["adaptive_selection_working"] = True
            print(f"âœ… å±‚æ¬¡åŒ–ä¸“å®¶æ¶æ„æ­£å¸¸å·¥ä½œ")
            print(f"   - ä¸“å®¶å±‚çº§: {test_results['num_levels']}")
            print(f"   - æ¯å±‚ä¸“å®¶æ•°: {test_results['experts_per_level']}")
            
        except Exception as e:
            print(f"âŒ å±‚æ¬¡åŒ–ä¸“å®¶æµ‹è¯•å¤±è´¥: {e}")
            test_results["error"] = str(e)
        
        return test_results
    
    def test_chart_aware_attention(self) -> Dict[str, Any]:
        """æµ‹è¯•å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›"""
        print("\nğŸ¯ æµ‹è¯•å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›...")
        
        test_results = {
            "chart_aware_attention_working": False,
            "chart_types_supported": [],
            "bias_patterns_generated": False,
            "flash_attention_available": False
        }
        
        try:
            # åˆ›å»ºå›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å—
            attention = ChartAwareAttention(hidden_size=768, num_heads=12)
            attention = attention.to(self.device)
            
            test_results["flash_attention_available"] = attention.use_flash_attn
            
            # æµ‹è¯•ä¸åŒå›¾è¡¨ç±»å‹çš„æ³¨æ„åŠ›åç½®
            chart_types = ['bar_chart', 'line_chart', 'pie_chart', 'scatter_plot', 'heatmap']
            seq_len = 64
            
            for chart_type in chart_types:
                try:
                    bias = attention._get_chart_bias(chart_type, seq_len, self.device)
                    if bias is not None:
                        test_results["chart_types_supported"].append(chart_type)
                except:
                    pass
            
            test_results["bias_patterns_generated"] = len(test_results["chart_types_supported"]) > 0
            
            # æµ‹è¯•æ³¨æ„åŠ›è®¡ç®—
            batch_size, seq_len = 2, 32
            hidden_size = 768
            
            query = torch.randn(batch_size, seq_len, hidden_size, device=self.device)
            key = torch.randn(batch_size, seq_len, hidden_size, device=self.device)
            value = torch.randn(batch_size, seq_len, hidden_size, device=self.device)
            
            output, weights = attention(query, key, value, chart_type='bar_chart')
            
            test_results["chart_aware_attention_working"] = output.shape == query.shape
            
            print(f"âœ… å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›æ­£å¸¸å·¥ä½œ")
            print(f"   - Flash Attention: {'å¯ç”¨' if test_results['flash_attention_available'] else 'ä¸å¯ç”¨'}")
            print(f"   - æ”¯æŒå›¾è¡¨ç±»å‹: {len(test_results['chart_types_supported'])}")
            
        except Exception as e:
            print(f"âŒ å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›æµ‹è¯•å¤±è´¥: {e}")
            test_results["error"] = str(e)
        
        return test_results
    
    def test_memory_management(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ç®¡ç†"""
        print("\nğŸ’¾ æµ‹è¯•å†…å­˜ç®¡ç†...")
        
        test_results = {
            "adaptive_memory_manager_working": False,
            "expert_loading_working": False,
            "memory_stats": {},
            "cache_efficiency": 0.0
        }
        
        try:
            # åˆ›å»ºè‡ªé€‚åº”å†…å­˜ç®¡ç†å™¨
            config = {
                "max_gpu_experts": 4,
                "memory_threshold": 0.8,
                "adaptive": True,
                "adaptation_interval": 10
            }
            
            memory_manager = AdaptiveMemoryManager(config)
            
            # æ¨¡æ‹Ÿä¸“å®¶åˆ—è¡¨
            mock_experts = [torch.nn.Linear(768, 768) for _ in range(8)]
            memory_manager.initialize(mock_experts)
            
            # æµ‹è¯•è‡ªé€‚åº”ä¸“å®¶åŠ è½½
            predicted_experts = [0, 1, 2, 5]
            current_complexity = 0.7
            
            loaded_experts = memory_manager.adaptive_expert_loading(
                predicted_experts, current_complexity
            )
            
            test_results["expert_loading_working"] = len(loaded_experts) > 0
            test_results["memory_stats"] = memory_manager.get_memory_stats()
            test_results["adaptive_memory_manager_working"] = True
            
            print(f"âœ… å†…å­˜ç®¡ç†æ­£å¸¸å·¥ä½œ")
            print(f"   - åŠ è½½ä¸“å®¶æ•°: {len(loaded_experts)}")
            print(f"   - å†…å­˜æ•ˆç‡: {test_results['memory_stats'].get('cache_efficiency', 0):.3f}")
            
        except Exception as e:
            print(f"âŒ å†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
            test_results["error"] = str(e)
        
        return test_results
    
    def test_inference_optimizations(self, model: ChartExpertMoE) -> Dict[str, Any]:
        """æµ‹è¯•æ¨ç†ä¼˜åŒ–"""
        print("\nâš¡ æµ‹è¯•æ¨ç†ä¼˜åŒ–...")
        
        test_results = {
            "early_exit_working": False,
            "kv_cache_working": False,
            "smart_inference_working": False,
            "performance_gains": {},
            "cache_stats": {}
        }
        
        try:
            image, input_ids, attention_mask = self.generate_test_data()
            
            # æµ‹è¯•æ™ºèƒ½æ¨ç†
            start_time = time.time()
            smart_outputs = model.smart_inference(
                image, input_ids, attention_mask,
                use_early_exit=True,
                use_kv_cache=True
            )
            smart_time = time.time() - start_time
            
            test_results["smart_inference_working"] = "logits" in smart_outputs
            test_results["performance_gains"]["smart_inference_time"] = smart_time
            
            # æµ‹è¯•æ—©æœŸé€€å‡ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(model, 'intermediate_confidence_estimators'):
                start_time = time.time()
                early_outputs = model.inference_with_early_exit(image, input_ids, attention_mask)
                early_time = time.time() - start_time
                
                test_results["early_exit_working"] = True
                test_results["performance_gains"]["early_exit_time"] = early_time
                test_results["early_exit_triggered"] = early_outputs.get("early_exit", False)
            
            # æµ‹è¯•KVç¼“å­˜
            if hasattr(model, 'kv_cache_pool'):
                start_time = time.time()
                cache_outputs = model.inference_with_kv_cache(image, input_ids, attention_mask)
                cache_time = time.time() - start_time
                
                test_results["kv_cache_working"] = True
                test_results["performance_gains"]["kv_cache_time"] = cache_time
                test_results["cache_stats"] = model.get_cache_stats()
            
            # å¯¹æ¯”æ ‡å‡†æ¨ç†
            start_time = time.time()
            standard_outputs = model.forward(image, input_ids, attention_mask)
            standard_time = time.time() - start_time
            
            test_results["performance_gains"]["standard_inference_time"] = standard_time
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            if smart_time > 0:
                speedup = standard_time / smart_time
                test_results["performance_gains"]["speedup_ratio"] = speedup
                
            print(f"âœ… æ¨ç†ä¼˜åŒ–æ­£å¸¸å·¥ä½œ")
            print(f"   - æ™ºèƒ½æ¨ç†: {'âœ“' if test_results['smart_inference_working'] else 'âœ—'}")
            print(f"   - æ—©æœŸé€€å‡º: {'âœ“' if test_results['early_exit_working'] else 'âœ—'}")
            print(f"   - KVç¼“å­˜: {'âœ“' if test_results['kv_cache_working'] else 'âœ—'}")
            if "speedup_ratio" in test_results["performance_gains"]:
                print(f"   - åŠ é€Ÿæ¯”: {test_results['performance_gains']['speedup_ratio']:.2f}x")
            
        except Exception as e:
            print(f"âŒ æ¨ç†ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
            test_results["error"] = str(e)
        
        return test_results
    
    def test_performance_benchmarks(self, model: ChartExpertMoE) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nğŸ“Š æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        test_results = {
            "throughput_tests": {},
            "memory_efficiency": {},
            "latency_tests": {},
            "scalability_tests": {}
        }
        
        # ååé‡æµ‹è¯•
        batch_sizes = [1, 2, 4, 8]
        seq_lengths = [32, 64, 128]
        
        for batch_size in batch_sizes:
            for seq_len in seq_lengths:
                if batch_size * seq_len > 512:  # é¿å…å†…å­˜æº¢å‡º
                    continue
                    
                try:
                    image, input_ids, attention_mask = self.generate_test_data(batch_size, seq_len)
                    
                    # é¢„çƒ­
                    with torch.no_grad():
                        _ = model.smart_inference(image, input_ids, attention_mask)
                    
                    # æµ‹è¯•
                    start_time = time.time()
                    num_runs = 5
                    
                    for _ in range(num_runs):
                        with torch.no_grad():
                            _ = model.smart_inference(image, input_ids, attention_mask)
                    
                    total_time = time.time() - start_time
                    avg_time = total_time / num_runs
                    throughput = batch_size / avg_time
                    
                    test_key = f"batch{batch_size}_seq{seq_len}"
                    test_results["throughput_tests"][test_key] = {
                        "avg_time": avg_time,
                        "throughput": throughput,
                        "samples_per_sec": throughput
                    }
                    
                except Exception as e:
                    print(f"âš ï¸  æ€§èƒ½æµ‹è¯•å¤±è´¥ (batch={batch_size}, seq={seq_len}): {e}")
        
        # å†…å­˜æ•ˆç‡æµ‹è¯•
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            
            image, input_ids, attention_mask = self.generate_test_data(4, 64)
            with torch.no_grad():
                _ = model.smart_inference(image, input_ids, attention_mask)
            
            peak_memory = torch.cuda.max_memory_allocated()
            memory_efficiency = (peak_memory - initial_memory) / (1024**3)  # GB
            
            test_results["memory_efficiency"] = {
                "peak_memory_gb": memory_efficiency,
                "efficiency_grade": "A" if memory_efficiency < 2.0 else "B" if memory_efficiency < 4.0 else "C"
            }
        
        print(f"âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        print(f"   - ååé‡æµ‹è¯•: {len(test_results['throughput_tests'])} ä¸ªé…ç½®")
        print(f"   - å†…å­˜æ•ˆç‡: {test_results.get('memory_efficiency', {}).get('efficiency_grade', 'N/A')}")
        
        return test_results
    
    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰ç»¼åˆæµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹ç»¼åˆä¼˜åŒ–æµ‹è¯•")
        print("=" * 60)
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_test_model()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        all_results = {
            "test_timestamp": time.time(),
            "device": str(self.device),
            "model_config": self.test_config
        }
        
        # 1. å±‚æ¬¡åŒ–ä¸“å®¶æµ‹è¯•
        all_results["hierarchical_experts"] = self.test_hierarchical_experts(model)
        
        # 2. å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›æµ‹è¯•
        all_results["chart_aware_attention"] = self.test_chart_aware_attention()
        
        # 3. å†…å­˜ç®¡ç†æµ‹è¯•
        all_results["memory_management"] = self.test_memory_management()
        
        # 4. æ¨ç†ä¼˜åŒ–æµ‹è¯•
        all_results["inference_optimizations"] = self.test_inference_optimizations(model)
        
        # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
        all_results["performance_benchmarks"] = self.test_performance_benchmarks(model)
        
        # 6. æ•´ä½“ä¼˜åŒ–ç»Ÿè®¡
        all_results["optimization_stats"] = model.get_optimization_stats()
        
        return all_results
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("ğŸ¯ ChartExpert-MoE ç»¼åˆä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 50)
        
        # æµ‹è¯•æ¦‚è§ˆ
        report.append(f"\nğŸ“Š æµ‹è¯•æ¦‚è§ˆ:")
        report.append(f"   è®¾å¤‡: {results['device']}")
        report.append(f"   æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(results['test_timestamp']))}")
        
        # ä¼˜åŒ–åŠŸèƒ½çŠ¶æ€
        report.append(f"\nğŸ”§ ä¼˜åŒ–åŠŸèƒ½çŠ¶æ€:")
        optimizations = results.get("optimization_stats", {}).get("optimizations_enabled", [])
        report.append(f"   å¯ç”¨çš„ä¼˜åŒ–: {', '.join(optimizations) if optimizations else 'æ— '}")
        
        # å±‚æ¬¡åŒ–ä¸“å®¶
        hier_results = results.get("hierarchical_experts", {})
        status = "âœ…" if hier_results.get("hierarchical_enabled", False) else "âŒ"
        report.append(f"   å±‚æ¬¡åŒ–ä¸“å®¶: {status}")
        if hier_results.get("hierarchical_enabled"):
            report.append(f"     - ä¸“å®¶å±‚çº§: {hier_results.get('num_levels', 0)}")
            report.append(f"     - æ¯å±‚ä¸“å®¶æ•°: {hier_results.get('experts_per_level', [])}")
        
        # å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›
        attn_results = results.get("chart_aware_attention", {})
        status = "âœ…" if attn_results.get("chart_aware_attention_working", False) else "âŒ"
        report.append(f"   å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›: {status}")
        if attn_results.get("chart_aware_attention_working"):
            report.append(f"     - Flash Attention: {'âœ…' if attn_results.get('flash_attention_available') else 'âŒ'}")
            report.append(f"     - æ”¯æŒå›¾è¡¨ç±»å‹: {len(attn_results.get('chart_types_supported', []))}")
        
        # å†…å­˜ç®¡ç†
        mem_results = results.get("memory_management", {})
        status = "âœ…" if mem_results.get("adaptive_memory_manager_working", False) else "âŒ"
        report.append(f"   è‡ªé€‚åº”å†…å­˜ç®¡ç†: {status}")
        
        # æ¨ç†ä¼˜åŒ–
        inf_results = results.get("inference_optimizations", {})
        report.append(f"   æ™ºèƒ½æ¨ç†: {'âœ…' if inf_results.get('smart_inference_working') else 'âŒ'}")
        report.append(f"   æ—©æœŸé€€å‡º: {'âœ…' if inf_results.get('early_exit_working') else 'âŒ'}")
        report.append(f"   KVç¼“å­˜: {'âœ…' if inf_results.get('kv_cache_working') else 'âŒ'}")
        
        # æ€§èƒ½ç»Ÿè®¡
        perf_results = results.get("performance_benchmarks", {})
        if perf_results.get("memory_efficiency"):
            mem_eff = perf_results["memory_efficiency"]
            report.append(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            report.append(f"   å†…å­˜æ•ˆç‡: {mem_eff.get('efficiency_grade', 'N/A')} ({mem_eff.get('peak_memory_gb', 0):.2f} GB)")
        
        if inf_results.get("performance_gains", {}).get("speedup_ratio"):
            speedup = inf_results["performance_gains"]["speedup_ratio"]
            report.append(f"   æ¨ç†åŠ é€Ÿ: {speedup:.2f}x")
        
        # ååé‡ç»Ÿè®¡
        throughput_tests = perf_results.get("throughput_tests", {})
        if throughput_tests:
            best_throughput = max(test["throughput"] for test in throughput_tests.values())
            report.append(f"   æœ€å¤§ååé‡: {best_throughput:.1f} samples/sec")
        
        report.append(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼æ‰€æœ‰æ¶æ„ä¼˜åŒ–å·²éªŒè¯ã€‚")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    tester = ComprehensiveOptimizationTester()
    
    try:
        # è¿è¡Œç»¼åˆæµ‹è¯•
        results = tester.run_comprehensive_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_report(results)
        print("\n" + report)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        os.makedirs("test_results", exist_ok=True)
        
        # ä¿å­˜JSONç»“æœ
        with open("test_results/comprehensive_optimization_results.json", "w") as f:
            json.dump(results, f, indent=2)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        with open("test_results/optimization_report.txt", "w") as f:
            f.write(report)
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° test_results/ ç›®å½•")
        
        # è®¡ç®—æ€»ä½“æˆåŠŸç‡
        total_optimizations = 6  # æ€»ä¼˜åŒ–æ•°é‡
        successful_optimizations = 0
        
        if results.get("hierarchical_experts", {}).get("hierarchical_enabled"):
            successful_optimizations += 1
        if results.get("chart_aware_attention", {}).get("chart_aware_attention_working"):
            successful_optimizations += 1
        if results.get("memory_management", {}).get("adaptive_memory_manager_working"):
            successful_optimizations += 1
        if results.get("inference_optimizations", {}).get("smart_inference_working"):
            successful_optimizations += 1
        if results.get("inference_optimizations", {}).get("early_exit_working"):
            successful_optimizations += 1
        if results.get("inference_optimizations", {}).get("kv_cache_working"):
            successful_optimizations += 1
        
        success_rate = (successful_optimizations / total_optimizations) * 100
        print(f"\nğŸ† æ€»ä½“æˆåŠŸç‡: {success_rate:.1f}% ({successful_optimizations}/{total_optimizations})")
        
        if success_rate >= 80:
            print("ğŸŒŸ ä¼˜åŒ–å®ç°ä¼˜ç§€ï¼æ‰€æœ‰ä¸»è¦åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        elif success_rate >= 60:
            print("ğŸ‘ ä¼˜åŒ–å®ç°è‰¯å¥½ï¼å¤§éƒ¨åˆ†åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        else:
            print("âš ï¸  éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œéƒ¨åˆ†åŠŸèƒ½å­˜åœ¨é—®é¢˜ã€‚")
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 