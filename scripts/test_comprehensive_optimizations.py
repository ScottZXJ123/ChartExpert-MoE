#!/usr/bin/env python3
"""
ChartExpert-MoE ç»¼åˆä¼˜åŒ–æµ‹è¯•è„šæœ¬

æµ‹è¯•æ‰€æœ‰å®ç°çš„æ¶æ„ä¼˜åŒ–ï¼š
1. å±‚æ¬¡åŒ–ä¸“å®¶æ¶æ„
2. å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›  
3. æ™ºèƒ½å†…å­˜ç®¡ç†
4. æ¨ç†ä¼˜åŒ–ï¼ˆæ—©æœŸé€€å‡ºã€KVç¼“å­˜ï¼‰
5. æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import torch
import time
import json
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

def test_optimizations():
    """æµ‹è¯•æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ChartExpert-MoE ç»¼åˆä¼˜åŒ–æµ‹è¯•")
    print(f"ğŸ“± è®¾å¤‡: {device}")
    print("=" * 50)
    
    # æµ‹è¯•é…ç½®
    config = {
        "hidden_size": 768,
        "vocab_size": 32000,
        "num_heads": 12,
        "use_hierarchical_experts": True,
        "use_flash_attention": True,
        "simple_confidence_threshold": 0.95,
        "medium_confidence_threshold": 0.90,
        "complex_confidence_threshold": 0.85,
        "num_early_exit_layers": 3,
        "kv_cache_size_limit": 1024 * 1024 * 1024,
        "vision_encoder": {
            "encoder_type": "mock",
            "hidden_size": 768,
            "model_name": "openai/clip-vit-base-patch32",
            "use_native_resolution": False,
            "use_2d_rope": False
        },
        "llm_backbone": {
            "model_name": "microsoft/DialoGPT-medium",
            "hidden_size": 4096,
            "use_mock": True
        },
        "fusion": {
            "hidden_size": 768, 
            "num_heads": 12,
            "fusion_type": "attention"
        },
        "routing": {
            "hidden_size": 768, 
            "num_experts": 12,
            "top_k": 2
        },
        "moe": {
            "hidden_size": 768,
            "num_experts": 12
        },
        "experts": {
            "layout": {"hidden_size": 768},
            "ocr": {"hidden_size": 768},
            "scale": {"hidden_size": 768},
            "geometric": {"hidden_size": 768},
            "trend": {"hidden_size": 768},
            "query": {"hidden_size": 768},
            "numerical": {"hidden_size": 768},
            "integration": {"hidden_size": 768},
            "alignment": {"hidden_size": 768},
            "chart_to_graph": {"hidden_size": 768},
            "shallow_reasoning": {"hidden_size": 768},
            "orchestrator": {"hidden_size": 768}
        }
    }
    
    results = {"timestamp": time.time(), "device": str(device)}
    
    try:
        # 1. æµ‹è¯•å±‚æ¬¡åŒ–ä¸“å®¶æ¶æ„
        print("ğŸ”„ æµ‹è¯•å±‚æ¬¡åŒ–ä¸“å®¶æ¶æ„...")
        from src.models.chart_expert_moe import ChartExpertMoE
        
        model = ChartExpertMoE(config)
        model = model.to(device)
        model.eval()
        
        has_hierarchical = hasattr(model, 'expert_levels')
        results["hierarchical_experts"] = {
            "enabled": has_hierarchical,
            "levels": len(model.expert_levels) if has_hierarchical else 0,
            "experts_per_level": [len(level) for level in model.expert_levels] if has_hierarchical else []
        }
        
        print(f"   âœ… å±‚æ¬¡åŒ–ä¸“å®¶: {'å¯ç”¨' if has_hierarchical else 'æœªå¯ç”¨'}")
        if has_hierarchical:
            print(f"   ğŸ“Š ä¸“å®¶å±‚çº§: {results['hierarchical_experts']['levels']}")
        
        # 2. æµ‹è¯•å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›
        print("ğŸ¯ æµ‹è¯•å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›...")
        from src.fusion.multimodal_fusion import ChartAwareAttention
        
        attention = ChartAwareAttention(768, 12)
        attention = attention.to(device)
        
        # æµ‹è¯•æ³¨æ„åŠ›è®¡ç®—
        query = torch.randn(2, 32, 768, device=device)
        key = torch.randn(2, 32, 768, device=device)
        value = torch.randn(2, 32, 768, device=device)
        
        output, _ = attention(query, key, value, chart_type='bar_chart')
        
        results["chart_aware_attention"] = {
            "working": output.shape == query.shape,
            "flash_attention": attention.use_flash_attn,
            "supported_charts": ['bar_chart', 'line_chart', 'pie_chart', 'scatter_plot', 'heatmap']
        }
        
        print(f"   âœ… å›¾è¡¨æ„ŸçŸ¥æ³¨æ„åŠ›: æ­£å¸¸å·¥ä½œ")
        print(f"   âš¡ Flash Attention: {'å¯ç”¨' if attention.use_flash_attn else 'ä¸å¯ç”¨'}")
        
        # 3. æµ‹è¯•å†…å­˜ç®¡ç†
        print("ğŸ’¾ æµ‹è¯•å†…å­˜ç®¡ç†...")
        from src.utils.memory_manager import AdaptiveMemoryManager
        
        mem_config = {
            "max_gpu_experts": 4,
            "memory_threshold": 0.8,
            "adaptive": True
        }
        
        memory_manager = AdaptiveMemoryManager(mem_config)
        mock_experts = [torch.nn.Linear(768, 768) for _ in range(8)]
        memory_manager.initialize(mock_experts)
        
        loaded_experts = memory_manager.adaptive_expert_loading([0, 1, 2, 5], 0.7)
        
        results["memory_management"] = {
            "adaptive_loading": len(loaded_experts) > 0,
            "loaded_experts": len(loaded_experts),
            "memory_stats": memory_manager.get_memory_stats()
        }
        
        print(f"   âœ… è‡ªé€‚åº”å†…å­˜ç®¡ç†: æ­£å¸¸å·¥ä½œ")
        print(f"   ğŸ“ˆ åŠ è½½ä¸“å®¶æ•°: {len(loaded_experts)}")
        
        # 4. æµ‹è¯•æ¨ç†ä¼˜åŒ–
        print("âš¡ æµ‹è¯•æ¨ç†ä¼˜åŒ–...")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        image = torch.randn(2, 3, 224, 224, device=device)
        input_ids = torch.randint(0, config["vocab_size"], (2, 64), device=device)
        attention_mask = torch.ones(2, 64, device=device)
        
        # æµ‹è¯•æ™ºèƒ½æ¨ç†
        start_time = time.time()
        smart_outputs = model.smart_inference(
            image, input_ids, attention_mask,
            use_early_exit=True,
            use_kv_cache=True
        )
        smart_time = time.time() - start_time
        
        # æµ‹è¯•æ ‡å‡†æ¨ç†
        start_time = time.time()
        standard_outputs = model.forward(image, input_ids, attention_mask)
        standard_time = time.time() - start_time
        
        speedup = standard_time / smart_time if smart_time > 0 else 1.0
        
        results["inference_optimizations"] = {
            "smart_inference": "logits" in smart_outputs,
            "early_exit_available": hasattr(model, 'intermediate_confidence_estimators'),
            "kv_cache_available": hasattr(model, 'kv_cache_pool'),
            "smart_time": smart_time,
            "standard_time": standard_time,
            "speedup": speedup
        }
        
        print(f"   âœ… æ™ºèƒ½æ¨ç†: æ­£å¸¸å·¥ä½œ")
        print(f"   ğŸƒ æ¨ç†åŠ é€Ÿ: {speedup:.2f}x")
        print(f"   ğŸ§  æ—©æœŸé€€å‡º: {'å¯ç”¨' if results['inference_optimizations']['early_exit_available'] else 'ä¸å¯ç”¨'}")
        print(f"   ğŸ’¿ KVç¼“å­˜: {'å¯ç”¨' if results['inference_optimizations']['kv_cache_available'] else 'ä¸å¯ç”¨'}")
        
        # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        # å†…å­˜æ•ˆç‡æµ‹è¯•
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            
            with torch.no_grad():
                _ = model.smart_inference(image, input_ids, attention_mask)
            
            peak_memory = torch.cuda.max_memory_allocated()
            memory_used = (peak_memory - initial_memory) / (1024**3)  # GB
            
            efficiency_grade = "A" if memory_used < 2.0 else "B" if memory_used < 4.0 else "C"
        else:
            memory_used = 0
            efficiency_grade = "N/A"
        
        # ååé‡æµ‹è¯•
        num_runs = 5
        start_time = time.time()
        
        for _ in range(num_runs):
            with torch.no_grad():
                _ = model.smart_inference(image, input_ids, attention_mask)
        
        total_time = time.time() - start_time
        throughput = (2 * num_runs) / total_time  # samples per second
        
        results["performance"] = {
            "memory_usage_gb": memory_used,
            "efficiency_grade": efficiency_grade,
            "throughput_samples_per_sec": throughput,
            "avg_inference_time": total_time / num_runs
        }
        
        print(f"   âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print(f"   ğŸ’¾ å†…å­˜æ•ˆç‡: {efficiency_grade} ({memory_used:.2f} GB)")
        print(f"   ğŸš„ ååé‡: {throughput:.1f} samples/sec")
        
        # 6. æ€»ä½“ä¼˜åŒ–ç»Ÿè®¡
        optimization_stats = model.get_optimization_stats()
        results["optimization_stats"] = optimization_stats
        
        print(f"\nğŸ”§ å¯ç”¨çš„ä¼˜åŒ–: {', '.join(optimization_stats.get('optimizations_enabled', []))}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        results["error"] = str(e)
        return results
    
    return results

def generate_report(results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report = []
    report.append("ğŸ¯ ChartExpert-MoE ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š")
    report.append("=" * 40)
    
    # è®¡ç®—æˆåŠŸç‡
    successful = 0
    total = 5
    
    if results.get("hierarchical_experts", {}).get("enabled"):
        successful += 1
    if results.get("chart_aware_attention", {}).get("working"):
        successful += 1
    if results.get("memory_management", {}).get("adaptive_loading"):
        successful += 1
    if results.get("inference_optimizations", {}).get("smart_inference"):
        successful += 1
    if results.get("performance", {}).get("efficiency_grade") in ["A", "B"]:
        successful += 1
    
    success_rate = (successful / total) * 100
    
    report.append(f"ğŸ“Š æ€»ä½“æˆåŠŸç‡: {success_rate:.1f}% ({successful}/{total})")
    report.append(f"âš¡ æ¨ç†åŠ é€Ÿ: {results.get('inference_optimizations', {}).get('speedup', 1.0):.2f}x")
    report.append(f"ğŸ’¾ å†…å­˜æ•ˆç‡: {results.get('performance', {}).get('efficiency_grade', 'N/A')}")
    report.append(f"ğŸš„ ååé‡: {results.get('performance', {}).get('throughput_samples_per_sec', 0):.1f} samples/sec")
    
    if success_rate >= 80:
        report.append("\nğŸŒŸ ä¼˜åŒ–å®ç°ä¼˜ç§€ï¼æ‰€æœ‰ä¸»è¦åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
    elif success_rate >= 60:
        report.append("\nğŸ‘ ä¼˜åŒ–å®ç°è‰¯å¥½ï¼å¤§éƒ¨åˆ†åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
    else:
        report.append("\nâš ï¸  éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œéƒ¨åˆ†åŠŸèƒ½å­˜åœ¨é—®é¢˜ã€‚")
    
    return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ChartExpert-MoEç»¼åˆä¼˜åŒ–æµ‹è¯•...\n")
    
    # è¿è¡Œæµ‹è¯•
    results = test_optimizations()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(results)
    print(f"\n{report}")
    
    # ä¿å­˜ç»“æœ
    os.makedirs("test_results", exist_ok=True)
    
    with open("test_results/comprehensive_optimization_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    with open("test_results/optimization_report.txt", "w") as f:
        f.write(report)
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° test_results/ ç›®å½•")

if __name__ == "__main__":
    main() 